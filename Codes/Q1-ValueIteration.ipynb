{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240f62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NSFrozenLake\n",
    "from amalearn.agent import AgentBase\n",
    "from draw_policy import draw_policy\n",
    "import numpy as np\n",
    "\n",
    "from CONFIG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea5e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(AgentBase):\n",
    "    \n",
    "    def __init__(self, id, environment, discount, theta, actions):\n",
    "        \n",
    "        self.environment = environment\n",
    "        self.actions = actions\n",
    "        self.i_limit, self.j_limit = 4, 4\n",
    "        self.n_states = self.i_limit*self.j_limit\n",
    "        \n",
    "        self.V = {}\n",
    "        self.Q = {}\n",
    "        self.init_V_Q()\n",
    "        \n",
    "        self.policy = {}\n",
    "        self.init_policy()\n",
    "        \n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        \n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "        \n",
    "    def init_V_Q(self):\n",
    "        \n",
    "        for i in range(self.i_limit):\n",
    "            for j in range(self.j_limit):\n",
    "                if (i, j) == (3, 3):\n",
    "                    self.V[(i, j)] = 0\n",
    "                else:\n",
    "                    self.V[(i, j)] = np.random.rand()\n",
    "                for a in self.actions:\n",
    "                    self.Q[((i, j), a)] = 0\n",
    "                \n",
    "    def init_policy(self):\n",
    "        \n",
    "        for i in range(self.i_limit):\n",
    "            for j in range(self.j_limit):\n",
    "                act = np.random.choice([i for i in range(len(ACTIONS))])\n",
    "                self.policy[(i, j)] = np.zeros(len(ACTIONS))\n",
    "                self.policy[(i, j)][act] = 1\n",
    "                \n",
    "    def calculate_v(self, state, action):\n",
    "        \n",
    "        states, probs, fail_probs, dones = self.environment.possible_consequences(action, state)\n",
    "        new_v = 0\n",
    "        for next_state, prob, fail_prob, done in zip(states, probs, fail_probs, dones):\n",
    "            new_v += prob*fail_prob*(FAIL_REWARD+MOVE_REWARD)\n",
    "            new_v += prob*(1-fail_prob)*(MOVE_REWARD + self.discount*self.V[next_state])\n",
    "            if done:\n",
    "                new_v += prob*(1-fail_prob)*(GOAL_REWARD)\n",
    "\n",
    "                \n",
    "        return new_v\n",
    "    \n",
    "    def value_iteration(self):\n",
    "        \n",
    "        epoch = 0\n",
    "        while True:\n",
    "            delta = 0\n",
    "            epoch += 1\n",
    "            for state, v in self.V.items():\n",
    "                \n",
    "                temp_v = v\n",
    "                max_v, max_act = 0, -1\n",
    "                for act in ACTIONS:\n",
    "                    new_v = self.calculate_v(state, act)\n",
    "                    self.Q[(state, act)] = new_v\n",
    "                    if new_v > max_v:\n",
    "                        max_v = new_v\n",
    "                        max_act = act\n",
    "                        \n",
    "                self.V[state] = max_v\n",
    "                \n",
    "                old_act = np.argmax(self.policy[state])\n",
    "                self.policy[state][old_act] = 0\n",
    "                self.policy[state][max_act] = 1\n",
    "                \n",
    "                delta = max(delta, abs(temp_v - max_v))\n",
    "             \n",
    "            if delta <= self.theta:\n",
    "                print(epoch)\n",
    "                break\n",
    "    \n",
    "    def take_action(self) -> (object, float, bool, object):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d1055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = NSFrozenLake(studentNum=STUDENT_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc60e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n"
     ]
    }
   ],
   "source": [
    "agent = Agent('1', environment, DISCOUNT, 0, ACTIONS)\n",
    "agent.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f708e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "| \u001b[44m0.000\u001b[0m | 0.001 | 0.332 | 0.746 | \n",
      "------------------------------\n",
      "| 0.696 | 0.001 | 0.143 | 0.998 | \n",
      "------------------------------\n",
      "| 0.703 | 0.001 | 0.001 | 0.001 | \n",
      "------------------------------\n",
      "| 0.861 | 0.401 | 0.128 | 0.000 | \n",
      "------------------------------\n",
      "→|↓|↓|←\n",
      "→|↓|↓|↓\n",
      "→|→|→|↓\n",
      "→|→|→|↻\n"
     ]
    }
   ],
   "source": [
    "environment.render()\n",
    "draw_policy(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f0ef847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== state (0, 0) ==================\n",
      "state: (0, 0), action: 0, Q-value: 194.4794221221824\n",
      "state: (0, 0), action: 1, Q-value: 70.4352357425524\n",
      "state: (0, 0), action: 2, Q-value: 220.23301811078974\n",
      "state: (0, 0), action: 3, Q-value: 194.4794221221824\n",
      "================== state (0, 1) ==================\n",
      "state: (0, 1), action: 0, Q-value: 198.02645484626086\n",
      "state: (0, 1), action: 1, Q-value: 252.29098315395277\n",
      "state: (0, 1), action: 2, Q-value: 147.41091822576334\n",
      "state: (0, 1), action: 3, Q-value: 223.7800508348682\n",
      "================== state (0, 2) ==================\n",
      "state: (0, 2), action: 0, Q-value: 219.216594896086\n",
      "state: (0, 2), action: 1, Q-value: 241.58331939898954\n",
      "state: (0, 2), action: 2, Q-value: 35.32279292750111\n",
      "state: (0, 2), action: 3, Q-value: 142.84746228698117\n",
      "================== state (0, 3) ==================\n",
      "state: (0, 3), action: 0, Q-value: 131.21772998055948\n",
      "state: (0, 3), action: 1, Q-value: -4.822973970027828\n",
      "state: (0, 3), action: 2, Q-value: 23.693060621079425\n",
      "state: (0, 3), action: 3, Q-value: 23.693060621079425\n",
      "================== state (1, 0) ==================\n",
      "state: (1, 0), action: 0, Q-value: 67.94898508737158\n",
      "state: (1, 0), action: 1, Q-value: 73.97721556140814\n",
      "state: (1, 0), action: 2, Q-value: 246.2576997746935\n",
      "state: (1, 0), action: 3, Q-value: 191.9931714670016\n",
      "================== state (1, 1) ==================\n",
      "state: (1, 1), action: 0, Q-value: 74.25052756186281\n",
      "state: (1, 1), action: 1, Q-value: 287.524898945157\n",
      "state: (1, 1), action: 2, Q-value: 246.4150344330037\n",
      "state: (1, 1), action: 3, Q-value: 224.04830993010017\n",
      "================== state (1, 2) ==================\n",
      "state: (1, 2), action: 0, Q-value: 250.06424881052598\n",
      "state: (1, 2), action: 1, Q-value: 328.04712069916053\n",
      "state: (1, 2), action: 2, Q-value: 9.143479931749214\n",
      "state: (1, 2), action: 3, Q-value: 145.18418388233655\n",
      "================== state (1, 3) ==================\n",
      "state: (1, 3), action: 0, Q-value: 241.93336440303847\n",
      "state: (1, 3), action: 1, Q-value: 368.20896399648353\n",
      "state: (1, 3), action: 2, Q-value: 7.15680334044278\n",
      "state: (1, 3), action: 3, Q-value: 35.672837931550035\n",
      "================== state (2, 0) ==================\n",
      "state: (2, 0), action: 0, Q-value: 70.28252209645052\n",
      "state: (2, 0), action: 1, Q-value: 20.323856567597446\n",
      "state: (2, 0), action: 2, Q-value: 277.5286630057082\n",
      "state: (2, 0), action: 3, Q-value: 64.25429162241397\n",
      "================== state (2, 1) ==================\n",
      "state: (2, 1), action: 0, Q-value: 80.47223093376245\n",
      "state: (2, 1), action: 1, Q-value: 176.01715366815432\n",
      "state: (2, 1), action: 2, Q-value: 330.73558703568244\n",
      "state: (2, 1), action: 3, Q-value: 252.7527151470478\n",
      "================== state (2, 2) ==================\n",
      "state: (2, 2), action: 0, Q-value: 298.73068586566285\n",
      "state: (2, 2), action: 1, Q-value: 340.2223195242302\n",
      "state: (2, 2), action: 2, Q-value: 383.8964209469546\n",
      "state: (2, 2), action: 3, Q-value: 257.62082135350954\n",
      "================== state (2, 3) ==================\n",
      "state: (2, 3), action: 0, Q-value: 339.1533619391056\n",
      "state: (2, 3), action: 1, Q-value: 435.9837179830394\n",
      "state: (2, 3), action: 2, Q-value: 381.30188182773503\n",
      "state: (2, 3), action: 3, Q-value: 20.249721171694265\n",
      "================== state (3, 0) ==================\n",
      "state: (3, 0), action: 0, Q-value: 16.00075508899516\n",
      "state: (3, 0), action: 1, Q-value: 16.00075508899516\n",
      "state: (3, 0), action: 2, Q-value: 161.5043433522401\n",
      "state: (3, 0), action: 3, Q-value: 65.95942061784824\n",
      "================== state (3, 1) ==================\n",
      "state: (3, 1), action: 0, Q-value: 30.05471567249338\n",
      "state: (3, 1), action: 1, Q-value: 175.5583039357383\n",
      "state: (3, 1), action: 2, Q-value: 328.7511557691715\n",
      "state: (3, 1), action: 3, Q-value: 287.2595221106041\n",
      "================== state (3, 2) ==================\n",
      "state: (3, 2), action: 0, Q-value: 187.78245929316526\n",
      "state: (3, 2), action: 1, Q-value: 340.9753111265984\n",
      "state: (3, 2), action: 2, Q-value: 439.33124870462717\n",
      "state: (3, 2), action: 3, Q-value: 342.50089266069335\n",
      "================== state (3, 3) ==================\n",
      "state: (3, 3), action: 0, Q-value: 349.1335697182676\n",
      "state: (3, 3), action: 1, Q-value: 447.4895072962964\n",
      "state: (3, 3), action: 2, Q-value: 447.4895072962964\n",
      "state: (3, 3), action: 3, Q-value: 392.807671140992\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(f'================== state {(i, j)} ==================')\n",
    "        for a in ACTIONS:\n",
    "            print(f'state: {(i, j)}, action: {a}, Q-value: {agent.Q[((i, j), a)]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
