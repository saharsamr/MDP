{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc610068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NSFrozenLake\n",
    "from amalearn.agent import AgentBase\n",
    "from draw_policy import draw_policy\n",
    "from plots import plot_total_episode_rewards, plot_regret, get_average\n",
    "from find_max_reward import find_max_reward\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from CONFIG import *\n",
    "EPISODES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e428f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(AgentBase):\n",
    "    \n",
    "    def __init__(self, id, environment, discount, epsilon, alpha, actions, episodes=1000, decade_lr=False):\n",
    "        \n",
    "        self.environment = environment\n",
    "        self.i_limit, self.j_limit = 4, 4\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.start_epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.start_alpha = alpha\n",
    "        self.decade_lr = decade_lr\n",
    "        self.discount = discount\n",
    "        self.episodes = episodes\n",
    "        \n",
    "        self.Q = {}\n",
    "        self.init_Q()\n",
    "        \n",
    "        self.target_policy = {}\n",
    "        self.b_policy = {}\n",
    "        self.update_policies()\n",
    "        \n",
    "        super(Agent, self).__init__(id, environment)\n",
    "        \n",
    "    def init_Q(self):\n",
    "        \n",
    "        for i in range(self.i_limit):\n",
    "            for j in range(self.j_limit):\n",
    "                for a in self.actions:\n",
    "                    if (i, j) == (3, 3):\n",
    "                        self.Q[((i, j), a)] = 0\n",
    "                    else:\n",
    "                        self.Q[((i, j), a)] = np.random.rand()\n",
    "                    \n",
    "    def update_state_t_policy(self, state):\n",
    "        \n",
    "        state_qs = [self.Q[(state, a)] for a in self.actions]\n",
    "        max_q_idx = np.argmax(state_qs)\n",
    "        \n",
    "        self.target_policy[state] = np.zeros(len(self.actions))\n",
    "        self.target_policy[state][max_q_idx] = 1\n",
    "        \n",
    "    def update_state_b_policy(self, state):\n",
    "        \n",
    "        state_qs = [self.Q[(state, a)] for a in self.actions]\n",
    "        max_q_idx = np.argmax(state_qs)\n",
    "        \n",
    "        self.b_policy[state] = np.zeros(len(self.actions))+(self.epsilon/len(self.actions))\n",
    "        self.b_policy[state][max_q_idx] += (1-self.epsilon)\n",
    "                    \n",
    "    def update_policies(self):\n",
    "        \n",
    "        for i in range(self.i_limit):\n",
    "            for j in range(self.j_limit):\n",
    "                self.update_state_t_policy((i, j))\n",
    "                self.update_state_b_policy((i, j))\n",
    "                \n",
    "    def update_q(self, state, next_state, action, reward):\n",
    "        \n",
    "        states, probs, _, __ = self.environment.possible_consequences(action, state_now=state)\n",
    "        other_states = sum([p*sum([self.b_policy[s][a]*self.Q[(s, a)] for a in self.actions]) \\\n",
    "                            for s, p in zip(states, probs) if s != next_state])\n",
    "        current_state = [p for s, p in zip(states, probs) if s == next_state][0]*self.b_policy[state][action]*reward\n",
    "        next_state_return = sum([self.b_policy[next_state][a]*self.Q[(next_state, a)] for a in self.actions])\n",
    "        \n",
    "        self.Q[(state, action)] += self.alpha*(other_states+current_state + self.discount*next_state_return - self.Q[state, action])\n",
    "                    \n",
    "    def q_learning(self):\n",
    "        \n",
    "        total_rewards = []\n",
    "        for epoch in range(REPS):\n",
    "            \n",
    "            self.reset()\n",
    "            episode_rewards = []\n",
    "            for e in range(self.episodes):\n",
    "                \n",
    "                rewards = []\n",
    "                state = self.environment.reset()\n",
    "                while True:\n",
    "                    \n",
    "                    action, next_state, reward, done = self.take_action(state)\n",
    "                    rewards.append(reward)\n",
    "                    \n",
    "                    max_next_q = max([self.Q[(next_state, a)] for a in self.actions])\n",
    "                    self.update_q(state, next_state, action, reward)\n",
    "                    self.update_state_b_policy(state)\n",
    "                    state = next_state\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                episode_rewards.append(sum(rewards))\n",
    "                \n",
    "                if (e+1)%40 == 0:\n",
    "                    self.epsilon *= 0.99\n",
    "                    \n",
    "            self.environment.reset()\n",
    "            total_rewards.append(episode_rewards)\n",
    "            \n",
    "        self.update_policies()\n",
    "        \n",
    "        return total_rewards\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        \n",
    "        action = np.random.choice(self.actions, p=self.b_policy[state])\n",
    "        next_state, reward, done, _ = self.environment.step(action)\n",
    "        \n",
    "        return action, next_state, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.init_Q()\n",
    "        self.update_policies()\n",
    "        self.epsilon = self.start_epsilon\n",
    "        self.alpha = self.start_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da3a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = NSFrozenLake(studentNum=STUDENT_NUM)\n",
    "max_expected_reward = find_max_reward(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb1087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent('1', environment, DISCOUNT, EPSILON, LEARNING_RATE, ACTIONS, episodes=EPISODES)\n",
    "rewards = agent.q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d60eca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------\n",
      "| \u001b[44m0.000\u001b[0m | 0.001 | 0.332 | 0.746 | \n",
      "------------------------------\n",
      "| 0.696 | 0.001 | 0.143 | 0.998 | \n",
      "------------------------------\n",
      "| 0.703 | 0.001 | 0.001 | 0.001 | \n",
      "------------------------------\n",
      "| 0.861 | 0.401 | 0.128 | 0.000 | \n",
      "------------------------------\n",
      "→|↓|←|→\n",
      "→|↑|↓|↓\n",
      "→|→|↓|↓\n",
      "←|→|→|↻\n"
     ]
    }
   ],
   "source": [
    "environment.render()\n",
    "draw_policy(agent.target_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b85251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
